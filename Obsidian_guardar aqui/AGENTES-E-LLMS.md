# ü§ñ Agentes do Sistema e Configura√ß√£o de LLMs

> **Data:** 2025-01-27  
> **Status:** Sistema otimizado com 14 agentes

---

## üìã AGENTES ATIVOS (14)

### 1. **Orchestrator** ‚úÖ
**Arquivo:** `src/agents/orchestrator.py`  
**LLM:** ‚úÖ **SIM** - Usa LLM para planejamento inteligente  
**Configura√ß√£o:**
- L√™ `LLM` do `.env` (padr√£o: `llama2`)
- Suporta: Ollama, OpenAI, AWS Bedrock
- **N√ÉO usa Gemini** (n√£o implementado no `load_llm()`)

**Uso do LLM:**
- Planejamento inteligente de tarefas
- Cria√ß√£o de planos em linguagem natural
- Execu√ß√£o de objetivos complexos

---

### 2. **System Health Agent** ‚úÖ
**Arquivo:** `src/agents/system_health_agent.py`  
**LLM:** ‚ùå **N√ÉO** - N√£o usa LLM diretamente  
**Funcionalidades:**
- Diagn√≥stico de problemas
- Monitoramento de agentes
- Gera√ß√£o de resolu√ß√µes (sem LLM)

---

### 3. **DB Manager** ‚úÖ
**Arquivo:** `src/agents/db_manager.py`  
**LLM:** ‚ùå **N√ÉO** - Apenas gerenciamento de bancos  
**Funcionalidades:**
- Conex√£o com bancos (Neo4j, Neon, MongoDB, Supabase)
- Execu√ß√£o de queries
- Gerenciamento de configura√ß√µes

---

### 4. **MCP Manager** ‚úÖ
**Arquivo:** `src/agents/mcp_manager.py`  
**LLM:** ‚ùå **N√ÉO** - Apenas gerenciamento de servidores MCP  
**Funcionalidades:**
- Gerenciamento de servidores MCP
- Health checks
- Listagem de recursos

---

### 5. **Git Integration** ‚úÖ
**Arquivo:** `src/agents/git_integration.py`  
**LLM:** ‚ùå **N√ÉO** - Apenas opera√ß√µes Git  
**Funcionalidades:**
- Opera√ß√µes Git/GitHub
- Commits, branches, PRs

---

### 6. **Neo4j GraphRAG** ‚úÖ
**Arquivo:** `src/agents/mcp_neo4j_integration.py`  
**LLM:** ‚úÖ **SIM** - Usa LLM para GraphRAG  
**Configura√ß√£o:**
- L√™ `LLM` do `.env` (padr√£o: `llama2`)
- Suporta: Ollama, OpenAI, AWS Bedrock
- **N√ÉO usa Gemini** (n√£o implementado)

**Uso do LLM:**
- Consultas GraphRAG
- Gera√ß√£o de respostas baseadas no grafo
- Busca sem√¢ntica

---

### 7. **Obsidian Integration** ‚úÖ
**Arquivo:** `src/agents/mcp_obsidian_integration.py`  
**LLM:** ‚ùå **N√ÉO** - Apenas gest√£o de notas  
**Funcionalidades:**
- Cria√ß√£o de notas
- Gest√£o de links
- Busca em notas

---

### 8. **Kestra Agent** ‚úÖ
**Arquivo:** `src/agents/mcp_kestra_integration.py`  
**LLM:** ‚ùå **N√ÉO** - Apenas cria√ß√£o de workflows  
**Funcionalidades:**
- Cria√ß√£o de workflows Kestra
- Agendamento de tarefas
- Gerenciamento de workflows

---

### 9. **Docker Integration** ‚úÖ
**Arquivo:** `src/agents/mcp_docker_integration.py`  
**LLM:** ‚ùå **N√ÉO** - Apenas detec√ß√£o de containers  
**Funcionalidades:**
- Detec√ß√£o de containers Docker
- Monitoramento de servi√ßos
- Informa√ß√µes de containers

---

### 10. **Streamlit Dashboard** ‚úÖ
**Arquivo:** `src/apps/agent_dashboard.py`  
**LLM:** ‚ö†Ô∏è **INDIRETO** - Via System Health Agent  
**Funcionalidades:**
- Interface visual
- Visualiza√ß√µes
- Chat com agentes

---

### 11. **MCP Manager UI** ‚úÖ
**Arquivo:** `src/agents/mcp_manager_ui.py`  
**LLM:** ‚ùå **N√ÉO** - Apenas interface  
**Funcionalidades:**
- Interface para MCP Manager
- Gerenciamento visual

---

### 12-14. **Agentes Deprecated** (Mantidos para compatibilidade)
- `diagnostic_agent.py` - Consolidado no System Health
- `resolution_agent.py` - Consolidado no System Health
- `agent_helper_system.py` - Consolidado no System Health

**LLM:** ‚ö†Ô∏è **PARCIAL** - `agent_helper_system.py` usa LLM para otimiza√ß√£o

---

## üîß CONFIGURA√á√ÉO DE LLM

### LLMs Suportados

O sistema usa `load_llm()` de `src/apps/chains.py` que suporta:

1. **Ollama** (Padr√£o) ‚úÖ
   - Modelo: `llama2` (configurado no `.env`)
   - Vari√°vel: `LLM=llama2`
   - URL: `OLLAMA_BASE_URL=http://localhost:11434`

2. **OpenAI** ‚ö†Ô∏è
   - Modelos: `gpt-4`, `gpt-4o`, `gpt-4-turbo`, `gpt-3.5`
   - Vari√°vel: `OPENAI_API_KEY` (n√£o configurada)

3. **AWS Bedrock** ‚ö†Ô∏è
   - Modelos: Claude, Titan, etc.
   - Vari√°vel: `AWS_ACCESS_KEY_ID` (comentada)

4. **Google Gemini** ‚ùå
   - **N√ÉO IMPLEMENTADO** no `load_llm()`
   - `GOOGLE_API_KEY` est√° configurada, mas s√≥ √© usada para **embeddings**
   - Embedding: `google-genai-embedding-001` ‚úÖ

---

## üìä RESUMO POR AGENTE

| Agente | Usa LLM? | Qual LLM? | Status |
|--------|----------|-----------|--------|
| **Orchestrator** | ‚úÖ Sim | Ollama (llama2) | Ativo |
| **Neo4j GraphRAG** | ‚úÖ Sim | Ollama (llama2) | Ativo |
| **Agent Helper System** | ‚úÖ Sim | Ollama (llama2) | Deprecated |
| **System Health** | ‚ùå N√£o | - | Ativo |
| **DB Manager** | ‚ùå N√£o | - | Ativo |
| **MCP Manager** | ‚ùå N√£o | - | Ativo |
| **Git Integration** | ‚ùå N√£o | - | Ativo |
| **Obsidian** | ‚ùå N√£o | - | Ativo |
| **Kestra** | ‚ùå N√£o | - | Ativo |
| **Docker** | ‚ùå N√£o | - | Ativo |
| **Dashboard** | ‚ö†Ô∏è Indireto | Via outros | Ativo |
| **MCP Manager UI** | ‚ùå N√£o | - | Ativo |

---

## ‚ö†Ô∏è GEMINI (Google)

### Status Atual
- ‚úÖ **GOOGLE_API_KEY** configurada: `AIzaSyD7lSqUzy-xvlP3sQHf0IaqAnemtgOqoeM`
- ‚úÖ **Embeddings** suportados: `google-genai-embedding-001`
- ‚ùå **LLM (Chat)** N√ÉO suportado no `load_llm()`

### O que Funciona
- ‚úÖ Embeddings do Google (se `EMBEDDING_MODEL=google-genai-embedding-001`)

### O que N√ÉO Funciona
- ‚ùå Usar Gemini como LLM principal
- ‚ùå Agentes n√£o usam Gemini para processamento

---

## üîß COMO ADICIONAR SUPORTE A GEMINI

Para usar Gemini como LLM, precisa adicionar no `load_llm()`:

```python
from langchain_google_genai import ChatGoogleGenerativeAI

def load_llm(llm_name: str, logger=BaseLogger(), config={}):
    # ... c√≥digo existente ...
    
    elif llm_name in ["gemini", "gemini-pro", "gemini-1.5-pro"]:
        logger.info(f"LLM: Using Google Gemini: {llm_name}")
        return ChatGoogleGenerativeAI(
            model=llm_name,
            temperature=0,
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
```

Depois, configurar no `.env`:
```bash
LLM=gemini-pro
GOOGLE_API_KEY=AIzaSyD7lSqUzy-xvlP3sQHf0IaqAnemtgOqoeM
```

---

## üìã CONFIGURA√á√ÉO ATUAL

### `.env` (Atual)
```bash
LLM=llama2                    # Ollama (padr√£o)
EMBEDDING_MODEL=sentence_transformer  # SentenceTransformer
GOOGLE_API_KEY=AIzaSyD7lSqUzy-xvlP3sQHf0IaqAnemtgOqoeM  # S√≥ para embeddings
OLLAMA_BASE_URL=http://localhost:11434
```

### Agentes que Usam LLM
1. **Orchestrator** ‚Üí `llama2` (Ollama)
2. **Neo4j GraphRAG** ‚Üí `llama2` (Ollama)
3. **Agent Helper System** ‚Üí `llama2` (Ollama)

---

## üéØ CONCLUS√ÉO

### LLM Atual
- **Padr√£o:** Ollama (`llama2`)
- **Configurado:** `LLM=llama2` no `.env`
- **Status:** ‚úÖ Funcionando

### Gemini
- **Embeddings:** ‚úÖ Suportado (mas n√£o est√° sendo usado)
- **LLM:** ‚ùå **N√ÉO suportado** (precisa adicionar c√≥digo)

### Para Usar Gemini
1. Adicionar suporte no `load_llm()`
2. Configurar `LLM=gemini-pro` no `.env`
3. Agentes automaticamente usar√£o Gemini

---

## üîó Links Relacionados

- [[PROJETO-IA-TEST|Projeto Principal]]
- [[SISTEMA-OTIMIZADO-FINAL|Sistema Otimizado]]
- [[Agentes/Orchestrator|Orchestrator]]

---

## üè∑Ô∏è Tags

#agentes #llm #gemini #ollama #configuracao

---

**√öltima atualiza√ß√£o:** 2025-01-27

